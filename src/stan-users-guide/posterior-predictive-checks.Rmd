# Posterior and Prior Predictive Checks {#ppcs.chapter}
{本节译者：陈苑莹}后验和先验预测检查

Posterior predictive checks are a way of measuring whether a model
does a good job of capturing relevant aspects of the data, such as
means, standard deviations, and quantiles [@Rubin:1984;
@GelmanEtAl:1996].  Posterior predictive checking works by simulating
new replicated data sets based on the fitted model parameters and then
comparing statistics applied to the replicated data set with the same
statistic applied to the original data set.

后验预测检查是一种衡量一个模型能否很好地捕获数据相关方面的方法，比如数据的均值、标准差和分位数等[@Rubin:1984;@GelmanEtAl:1996]。该方法的工作原理是根据拟合的模型参数模拟新的复制数据集，然后将应用于复制数据集的统计信息与应用于原始数据集的相同统计信息进行比较。

Prior predictive checks evaluate the prior the same way.
Specifically, they evaluate what data sets would be consistent with
the prior.  They will not be calibrated with actual data, but extreme
values help diagnose priors that are either too strong, too weak,
poorly shaped, or poorly located.

先验预测检查用同样的方式评估先验信息。具体来说，是评估哪些数据集与先验信息一致。 它们不会使用实际数据进行校准，但极值有助于诊断太强、太弱、形状差，或位置差。

Prior and posterior predictive checks are two cases of the general
concept of predictive checks, just conditioning on different things
(no data and the observed data, respectively).  For hierarchical
models, there are intermediate versions, as discussed in the section
on [hierarchical models and mixed replication](#mixed-replication).

先验和后验预测检查是预测性检查一般概念的两种情况，只是以不同事物为条件（分别是无数据和观察数据）。对于分层模型，存在中间版本，如 [分层模型和混合复制] 一节中所述。

## Simulating from the posterior predictive distribution
后验预测分布的模拟

The posterior predictive distribution is the distribution over new
observations given previous observations.  It's predictive in the
sense that it's predicting behavior on new data that is not part of
the training set.  It's posterior in that everything is conditioned on
observed data $y$.

后验预测分布是给定先前观测值的新观测值的分布。从某种意义上说，它是预测性的，因为它预测了不属于训练集的新数据的行为。它是后验的，因为一切都取决于观察到的数据$y$。

The posterior predictive distribution for replications
$y^{\textrm{rep}}$ of the original data set $y$ given model parameters
$\theta$ is defined by
$$
p(y^{\textrm{rep}} \mid y)
= \int p(y^{\textrm{rep}} \mid \theta)
       \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$

给定模型参数$\theta$的原始数据集$y$的重复$y^{\textrm{rep}}$的后验预测分布定义为
$$
p(y^{\textrm{rep}} \mid y)
= \int p(y^{\textrm{rep}} \mid \theta)
       \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$

As with other posterior predictive quantities, generating a replicated
data set $y^{\textrm{rep}}$ from the posterior predictive distribution is
straightforward using the generated quantities block.  Consider a simple regression
model with parameters $\theta = (\alpha, \beta, \sigma).$

与其他后验预测量一样，使用生成的数量块可以直接从后验预测分布生成一个复制的数据集$y^{\textrm{rep}}$。考虑一个带参数的简单回归模型$\theta = (\alpha, \beta, \sigma)$。
```stan
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  alpha ~ normal(0, 2);
  beta ~ normal(0, 1);
  sigma ~ normal(0, 1);
  y ~ normal(alpha + beta * x, sigma);
}
```
To generate a replicated data set `y_rep` for this simple model, the
following generated quantities block suffices.

要为这个简单模型生成一个复制的数据集`y_rep`，使用下面这个"generated quantities"代码块就足够了。
```stan
generated quantities {
  array[N] real y_rep = normal_rng(alpha + beta * x, sigma);
}
```
The vectorized form of the normal random
number generator is used with the original predictors `x` and the
model parameters `alpha, beta`, and `sigma.`
The replicated data variable `y_rep` is declared to be the same size
as the original data `y`, but instead of a vector type, it is
declared to be an array of reals to match
the return type of the function `normal_rng`.
Because the vector and real array types have the same dimensions and layout,
they can be plotted against one another and otherwise compared during
downstream processing.

正态随机数生成器的向量化形式与原始预测量`x`和模型参数`alpha, beta`和`sigma`一起使用。复制的数据变量`y_rep`与原始数据`y`维度相同，但它不是向量类型，而是一个实数矩阵，以匹配函数`normal_rng`的返回类型。由于向量和实数矩阵具有相同的维度和结果，因此它们可以相互绘制，或者在后续处理时进行比较。

The posterior predictive
sampling for posterior predictive checks is different from usual
posterior predictive sampling discussed in [the chapter on posterior
predictions](#posterior-prediction.chapter) in that the original
predictors $x$ are used.  That is, the posterior predictions are for
the original data.

后验预测检查的后验预测抽样与[后验预测一章](#posterior-prediction.chapter)中讨论的通常后验预测抽样不同，因为那章使用的是原始预测因子 $x$。也就是说，后验预测是针对原始数据的。

## Plotting multiples
绘制乘子

A standard posterior predictive check would plot a histogram of each
replicated data set along with the original data set and compare them
by eye.  For this purpose, only a few replications are needed.  These
should be taken by thinning a larger set of replications down to the
size needed to ensure rough independence of the replications.

标准的后验预测检查将绘制每个复制数据集与原始数据集的直方图并直观地进行比较。为此，只需要少量的复制集，这可以通过将一个较大的复制集精简到能确保基本独立性的规格大小。

Here's a complete example where the model is a simple Poisson with
a weakly informative exponential prior with a mean of 10 and
standard deviation of 10.

下面是一个完整的示例，其中模型是具有弱信息指数先验的简单泊松，平均值为 10，标准差为 10。

```stan
data {
  int<lower=0> N;
  array[N] int<lower=0> y;
}
transformed data {
  real<lower=0> mean_y = mean(to_vector(y));
  real<lower=0> sd_y = sd(to_vector(y));
}
parameters {
  real<lower=0> lambda;
}
model {
  y ~ poisson(lambda);
  lambda ~ exponential(0.2);
}
generated quantities {
  array[N] int<lower=0> y_rep = poisson_rng(rep_array(lambda, N));
  real<lower=0> mean_y_rep = mean(to_vector(y_rep));
  real<lower=0> sd_y_rep = sd(to_vector(y_rep));
  int<lower=0, upper=1> mean_gte = (mean_y_rep >= mean_y);
  int<lower=0, upper=1> sd_gte = (sd_y_rep >= sd_y);
}
```
The generated quantities block creates a variable `y_rep` for the
replicated data, variables `mean_y_rep` and `sd_y_rep` for the
statistics of the replicated data, and indicator variables
`mean_gte` and `sd_gte` for whether the replicated statistic
is greater than or equal to the statistic applied to the original
data.

“generated quantities”代码块为复制的数据创建了变量`y_rep`、为复制数据的统计量创建了变量`mean_y_rep`和`sd_y_rep`，还为复制的统计量是否大于或等于原始数据的统计量创建了示性变量`mean_gte`和`sd_gte`。

Now consider generating data $y \sim \textrm{Poisson}(5)$.  The
resulting small multiples plot shows the original data plotted in the
upper left and eight different posterior replications plotted in the
remaining boxes.

现在考虑生成数据$y \sim \textrm{Poisson}(5)$。生成的小倍数图显示，原始数据绘制在左上角，其余框中绘制了八个不同的后验重复。

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Posterior predictive checks for Poisson data generating process and Poisson model.（对于泊松数据生成过程和泊松模型的后验分布检查）"}
knitr::include_graphics("./img/ppc-pois-pois.jpg", auto_pdf = TRUE)
```

With a Poisson data-generating process and Poisson model, the
posterior replications look similar to the original data.  If it were
easy to pick the original data out of the lineup, there would be a
problem.

使用泊松数据生成过程和泊松模型，后验复制看起来与原始数据相似。如果很容易从队列中挑选出原始数据，那么就会出现问题。

Now consider generating over-dispersed data $y \sim \textrm{negative-binomial2}(5, 1).$  This has the same mean as
$\textrm{Poisson}(5)$, namely $5$, but a standard deviation of
$\sqrt{5 + 5^2 /1} \approx 5.5.$ There is no way to fit this data with
the Poisson model, because a variable distributed as
$\textrm{Poisson}(\lambda)$ has mean $\lambda$ and standard deviation
$\sqrt{\lambda},$ which is $\sqrt{5}$ for $\textrm{Poisson}(5).$
Here's the resulting small multiples plot, again with original data in
the upper left.

现在考虑生成过度分散化的数据$y \sim \textrm{negative-binomial2}(5, 1)$，与$\textrm{Poisson}(5)$有相同的均值$5$，但是标准差不同为$\sqrt{5 + 5^2 /1} \approx 5.5$。此时无法用泊松模型去拟合这个数据，因为服从$\textrm{Poisson}(\lambda)$的变量均值为$\lambda$，标准差为$\sqrt{\lambda}$。下图是小倍数结果图，同样原始数据在左上角。

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.cap = "Posterior predictive checks for negative binomial data generating process and Poisson model.（对于负二项分布数据过程和泊松模型的后验预测分布）"}
knitr::include_graphics("./img/ppc-nb-pois.jpg", auto_pdf = TRUE)
```

This time, the original data stands out in stark contrast to the
replicated data sets, all of which are clearly more symmetric and
lower variance than the original data.  That is, the model's not
appropriately capturing the variance of the data.

这一次，原始数据与复制数据集形成鲜明对比，所有复制数据集都明显比原始数据更对称，方差更低。也就是说，模型没有适当地捕获数据的方差。

## Posterior ''p-values''
后验“p值”

If a model captures the data well, summary statistics such as
sample mean and standard deviation, should have similar values in
the original and replicated data sets.  This can be tested
by means of a p-value-like statistic, which here is just the probability the
test statistic $s(\cdot)$ in a replicated data set exceeds that in
the original data,

如果一个模型很好地捕获了数据，那么汇总统计量(如样本均值和标准差)在原始和复制数据集中应该具有相似的值。这可以通过类p值统计量来检验，这里是复制数据集的检验统计量$s(\cdot)$超过原始数据集的概率，
$$
\textrm{Pr}\left[ s(y^{\textrm{rep}}) \geq s(y) \mid y \right]
=
\int
\textrm{I}\left( s(y^{\textrm{rep}}) \geq s(y) \mid y \right)
\cdot p\left( y^{\textrm{rep}} \mid y \right)
\, \textrm{d}{y^{\textrm{rep}}}.
$$

It is important to note that''p-values'' is in quotes because these
statistics are not classically calibrated, and thus will not in
general have a uniform distribution even when the model is well
specified [@BayarriBerger:2000].

需要注意的是，“p值”是带引号的，因为这些统计量没有经过经典校准，因此即使模型很好地指定，通常也不会具有均匀分布[@BayarriBerger:2000]。

Nevertheless, values of this statistic very close to zero or
one are cause for concern that the model is not fitting the data well.
Unlike a visual test, this p-value-like test is easily automated for
bulk model fitting.

然而，该统计量非常接近于0或1的值引起了人们的关注，即模型不能很好地拟合数据。与视觉检验不同，这种类p值的检验很容易自动用于批量模型拟合。

To calculate event probabilities in Stan, it suffices to define
indicator variables that take on value 1 if the event occurs and
0 if it does not.  The posterior mean is then the event probability.
For efficiency, indicator variables are defined in the
generated quantities block.

要在Stan中计算事件概率，定义示性变量就足够了，如果事件发生值为1，如果事件没有发生值为0。后验均值就是事件概率。为了提高效率，在“generated quantities”代码块中定义了示性变量。
```stan
generated quantities {
  int<lower=0, upper=1> mean_gt;
  int<lower=0, upper=1> sd_gt;
  {
    array[N] real y_rep = normal_rng(alpha + beta * x, sigma);
    mean_gt = mean(y_rep) > mean(y);
    sd_gt = sd(y_rep) > sd(y);
  }
}
```
The indicator variable `mean_gt` will have value 1 if the mean of the
simulated data `y_rep` is greater than or equal to the mean of he
original data `y`. Because the values of `y_rep` are not needed for
the posterior predictive checks, the program saves output space by
using a local variable for `y_rep`.  The statistics `mean(u)` and
`sd(y)` could also be computed in the transformed data block and
saved.

示性变量`mean_gt`值为1如果模拟数据`y_rep`的均值大于或等于原始数据`y`的均值。因为对于后验预测检查不需要`y_rep`的值，因此该程序通过对于`y_rep`使用一个局部变量来节省输出空间。统计量`mean(u)`和`sd(y)` 在“transformed data”代码块中也可以被计算和保存。 

For the example in the previous section, where over-dispersed
data generated by a negative binomial distribution was fit with a
simple Poisson model, the following plot illustrates the posterior
p-value calculation for the mean statistic.

对于上一节中的例子，负二项分布产生的过度分散的数据用一个简单的泊松模型拟合，下图说明了均值统计量的后验p值计算。

```{r include = TRUE, echo = FALSE, fig.align = "center", out.width = "50%", fig.cap = "Histogram of means of replicated data sets; vertical red line at mean of original data.（复制数据集的均值直方图；垂直的红线为原始数据的均值）"}
knitr::include_graphics("./img/ppc-pvalue-nb-pois-mean.jpg", auto_pdf = TRUE)
```

The p-value for the mean is just the percentage of replicated data
sets whose statistic is greater than or equal that of the original
data.  Using a Poisson model for negative binomial data still fits the
mean well, with a posterior $p$-value of 0.49.  In Stan terms, it is
extracted as the posterior mean of the indicator variable `mean_gt`.

均值的p-值就是复制数据集的统计量大于或等于原始数据集的百分比。对负二项数据使用泊松模型仍然可以很好地拟合均值，后验$p-$值为0.49。在Stan术语中，它被抽象为示性变量`mean_gt`的后验均值。

The standard deviation statistic tells a different story.

标准差统计量呈现了一个截然不同的结果。

```{r include = TRUE, echo = FALSE, fig.align = "center", out.width = "50%", fig.cap = "Scatterplot of standard deviations of replicated data sets; the vertical red line is at standard deviation of original data.（重复数据集标准差的散点图；垂直的红线表示原始数据的标准差）"}
knitr::include_graphics("./img/ppc-pvalue-nb-pois-sd.jpg", auto_pdf = TRUE)
```

Here, the original data has much higher standard deviation than any of
the replicated data sets.  The resulting $p$-value estimated by Stan
after a large number of iterations is exactly zero (the absolute error
bounds are fine, but a lot of iterations are required to get good
relative error bounds on small $p$-values by sampling).  In other
words, there were no posterior draws in which the replicated data set
had a standard deviation greater than or equal to that of the original
data set.  Clearly, the model is not capturing the dispersion of the
original data.  The point of this exercise isn't just to figure out
that there's a problem with a model, but to isolate where it is.
Seeing that the data is over-dispersed compared to the Poisson model
would be reason to fit a more general model like the negative binomial
or a latent varying effects (aka random effects) model that can
account for the over-dispersion.

这里，原始数据的标准差比任何复制数据集都要高得多。因此经过大量迭代后Stan估计产生的$p-$值恰好为零(绝对误差范围很好，但需要大量迭代才能通过采样得到小p值的良好相对误差范围)。换句话说，没有后验能导致复制数据集的标准差大于或等于原始数据集的标准差。显然，该模型没有捕捉到原始数据的离散性。这个练习的重点不仅仅是找出模型有问题，而是找出问题所在。看到数据与泊松模型相比过于分散，就有理由拟合一个更一般的模型，如负二项或可以解释过度分散的潜在变化效应(又名随机效应)模型。

### Which statistics to test?
检验哪个统计量？

Any statistic may be used for the data, but these can be guided by the
quantities of interest in the model itself.  Popular choices in
addition to mean and standard deviation are quantiles, such as the
median, 5% or 95% quantiles, or even the maximum or minimum value to
test extremes.

任何统计量都可以用于数据，但这些统计量可以由模型本身中感兴趣的量来确定。除了均值和标准差之外，热门的选择是分位数，例如中位数、5% 或 95% 分位数，还有检验极端值的最大值或最小值。

Despite the range of choices, test statistics should ideally be
ancillary, in the sense that they should be testing something other
than the fit of a parameter.  For example, a simple normal model of a
data set will typically fit the mean and variance of the data quite
well as long as the prior doesn't dominate the posterior.  In
contrast, a Poisson model of the same data cannot capture both the
mean and the variance of a data set if they are different, so they
bear checking in the Poisson case.  As we saw with the Poisson case,
the posterior mean for the single rate parameter was located near the
data mean, not the data variance.  Other distributions such as the
lognormal and gamma distribution, have means and variances that are
functions of two or more parameters.

尽管有选择范围，但理想情况下，检验统计量应该是辅助的，它们应该检验参数拟合度以外的其他内容。例如，一个简单的正态模型通常可以很好地拟合数据的均值和方差，只要先验不主导后验。相反，如果数据集的均值方差不同，则一个泊松模型就无法同时捕获数据的均值和方差，因此它们需要在泊松情况下进行检查。正如我们在泊松情况下看到的那样，单个比例参数的后验均值位于数据均值附近，而不在数据方差附近。其他分布（如对数正态分布和伽马分布）的均值和方差是两个或多个参数的函数。

## Prior predictive checks
先验预测检查

Prior predictive checks generate data according to the prior in order
to asses whether a prior is appropriate [@GabryEtAl:2019].  A
posterior predictive check generates replicated data according to the
posterior predictive distribution.  In contrast, the prior predictive
check generates data according to the prior predictive distribution,

先验预测检查根据先验生成数据，以评估先验是否合适[@GabryEtAl:2019]。一个后验预测检查根据后验预测分布生成重复数据。相比之下，先验预测检查根据先验预测分布生成数据，
$$
y^{\textrm{sim}} \sim p(y).
$$
The prior predictive distribution is just like the posterior
predictive distribution with no observed data, so that a prior
predictive check is nothing more than the limiting case of a posterior
predictive check with no data.

先验预测分布就像没有观测数据的后验预测分布，因此，先验预测检查只不过是没有数据的后验预测检查的极限情况。

This is easy to carry out mechanically by simulating parameters

这很容易通过模拟参数来程序化地实现
$$
\theta^{\textrm{sim}} \sim p(\theta)
$$
according to the priors, then simulating data

根据先验信息，再模拟数据
$$
y^{\textrm{sim}} \sim p(y \mid \theta^{\textrm{sim}})
$$
according to the sampling distribution given the simulated
parameters.  The result is a simulation from the joint
distribution,

根据给出了模拟参数的抽样分布。结果是对联合分布的模拟，
$$
(y^{\textrm{sim}}, \theta^{\textrm{sim}}) \sim p(y, \theta)
$$
and thus

因此
$$
y^{\textrm{sim}} \sim p(y)
$$
is a simulation from the prior predictive distribution.

是一个来自先验预测分布的模拟。

### Coding prior predictive checks in Stan
在Stan中进行先验预测检查的编程

A prior predictive check is coded just like a posterior predictive
check.  If a posterior predictive check has already been coded and
it's possible to set the data to be empty, then no additional coding
is necessary.  The disadvantage to coding prior predictive checks as
posterior predictive checks with no data is that Markov chain Monte
Carlo will be used to sample the parameters, which is less efficient
than taking independent draws using random number generation.

先验预测检查就像后验预测检查一样编写代码。如果一个后验预测检查代码已经被编写，并且可以将数据设置为空，则不需要额外的编程。将先验预测检查编写为没有数据的后验预测检查的缺点是使用马尔可夫链蒙特卡罗对参数进行采样，这比使用随机数生成独立样本效率低。

Prior predictive checks can be coded entirely within the generated
quantities block using random number generation.  The resulting draws
will be independent.  Predictors must be read in from the actual data
set---they do not have a generative model from which to be simulated.
For a Poisson regression, prior predictive sampling can be
encoded as the following complete Stan program.

先验预测检查可以完全在使用随机数生成器的“generated quantities”代块内编写，结果将是独立的。预测器必须从实际数据集中读取——它们没有一个生成模型来进行模拟。对于一个泊松回归，先验预测抽样可以编写为以下完整的Stan程序。
```stan
data {
  int<lower=0> N;
  vector[N] x;
}
generated quantities {
  real alpha = normal_rng(0, 1);
  real beta = normal_rng(0, 1);
  array[N] real y_sim = poisson_log_rng(alpha + beta * x);
}
```
Running this program using Stan's fixed-parameter sampler
yields draws from the prior.  These may be plotted to
consider their appropriateness.

使用Stan的固定参数采样器运行这个程序会产生来自先验的结果。这些可能会考虑到它们的适当性。

## Example of prior predictive checks
一个关于先验预测检查的例子

Suppose we have a model for a football (aka soccer) league where there
are $J$ teams.  Each team has a scoring rate $\lambda_j$ and in each
game will be assumed to score $\textrm{poisson}(\lambda_j)$ points.
Yes, this model completely ignores defense.  Suppose the modeler does
not want to "put their thumb on the scale" and would rather "let the
data speak for themselves" and so uses a prior with very wide tails,
because it seems uninformative, such as the widely deployed

假设我们有一个足球联赛的模型，其中有
$J$个队伍。每个队都有一个得分率$λ_j$以及在每场比赛中的得分认为服从$\textrm{poisson}(\lambda_j)$。是的，该模型完全忽略防守。假设建模者不想“把他们的拇指放在刻度上”，而宁愿“让数据自己说话”，因此使用非常宽尾的先验，因为它看起来没有信息，比如说
$$
\lambda_j \sim \textrm{gamma}(\epsilon_1, \epsilon_2).
$$
This is not just a manufactured example;  *The BUGS Book*
recommends setting $\epsilon = (0.5, 0.00001)$, which
corresponds to a Jeffreys prior for a Poisson rate parameter prior
[@LunnEtAl:2012, p. 85].

这不仅仅是一个人为的例子；*The BUGS Book*建议设置$\epsilon = (0.5, 0.00001)$
，这相当于一个泊松率参数先验的Jeffreys先验[@LunnEtAl:2012, p. 85]。

Suppose the league plays a round-robin tournament wherein every team
plays every other team.  The following Stan model generates random team
abilities and the results of such a round-robin tournament, which may
be used to perform prior predictive checks.

假设联盟进行循环赛，每支球队都会与其他每支球队进行比赛。下面的Stan模型随机生成球队的能力和这种循环赛的结果，可以用来展示先验预测检查的效果。
```stan
data {
  int<lower=0> J;
  array[2] real<lower=0> epsilon;
}
generated quantities {
  array[J] real<lower=0> lambda;
  array[J, J] int y;
  for (j in 1:J) lambda[j] = gamma_rng(epsilon[1], epsilon[2]);
  for (i in 1:J) {
    for (j in 1:J) {
      y[i, j] = poisson_rng(lambda[i]) - poisson_rng(lambda[j]);
    }
  }
}
```
In this simulation, teams play each other twice and play themselves
once.  This could be made more realistic by controlling the
combinatorics to only generate a single result for each pair of teams,
of which there are $\binom{J}{2} = \frac{J \cdot (J - 1)}{2}.$

在这个模拟中，各支队伍对战对方两次，对战自己一次。这可以通过控制组合运算，使每对队伍只产生一个组合结果$\binom{J}{2} = \frac{J \cdot (J - 1)}{2}$来实现。

Using the $\textrm{gamma}(0.5, 0.00001)$ reference prior on team
abilities, the following are the first 20 simulated point differences
for the match between the first two teams, $y^{(1:20)}_{1, 2}$.

使用$\textrm{gamma}(0.5, 0.00001)$作为队伍能力的先验分布，下面是前两个队伍比赛的前20个模拟分差，$y^{(1:20)}_{1, 2}$。
```
2597 -26000   5725  22496   1270   1072   4502  -2809   -302   4987
7513   7527  -3268 -12374   3828   -158 -29889   2986  -1392     66
```
That's some pretty highly scoring football games being simulated; all
but one has a score differential greater than 100!  In other words, this
$\textrm{gamma}(0.5, 0.00001)$ prior is putting around 95% of its
weight on score differentials above 100.  Given that two teams
combined rarely score 10 points, this prior is way out of line with
prior knowledge about football matches; it is not only consistent with
outcomes that have never occurred in the history of the sport, it puts
most of the prior probability mass there.

这是模拟了一些得分很高的足球比赛；除了一个以外，所有的分差都大于100！换句话说，这个$\textrm{gamma}(0.5, 0.00001)$先验分布将95%的权重放在100分以上的分差上。考虑到两支球队加起来很少能得10分，这个先验与足球比赛的先验知识是不一致的；它不仅与体育运动历史上从未发生过的结果一致，而且把大部分先验概率都放在了那里。

The posterior predictive distribution can be strongly affected by the
prior when there is not much observed data and substantial prior mass
is concentrated around infeasible values [@Gelman:2006].

当观测数据不多，先验分布大量集中在不可行值周围时，后验预测分布会受到先验的强烈影响[@Gelman:2006]。

Just as with posterior predictive distributions, any statistics of the
generated data may be evaluated.  Here, the focus was on score
difference between a single pair of teams, but it could've been on
maximums, minimums, averages, variances, etc.

就像后验预测分布一样，数据的任何统计量都可以被评估。在这里，我们关注的是两支球队之间的得分差异，但也可以是最大值、最小值、平均值、方差等。

In this textbook example, the prior is univariate and directly related
to the expected number of points scored, and could thus be directly
inspected for consistency with prior knowledge about scoring rates in
football.  There will not be the same kind of direct connection when
the prior and sampling distributions are multivariate.  In these more
challenging situations, prior predictive checks are an easy way to get
a handle on the implications of a prior in terms of what it says the
data is going to look like;  for a more complex application involving
spatially heterogeneous air pollution concentration, see [@GabryEtAl:2019].

在此教材示例中，先验分布是单变量的，并且与得分的预期数直接相关，因此可以直接检查关于足球得分率的先验知识的一致性。当先验分布和抽样分布是多元分布时，不会有类似的直接联系。在这些更具挑战性的情况下，先验预测检查是一种可以处理先验含义的简单方法，即它所描述的数据将是什么样子；可以用于涉及空间异构空气污染浓度的更复杂应用，见[@GabryEtAl:2019]。

Prior predictive checks can also be compared with the data, but one
should not expect them to be calibrated in the same way as posterior
predictive checks.  That would require guessing the posterior and
encoding it in the prior.  The goal is make sure the prior is not so
wide that it will pull probability mass away from feasible values.

先验预测检查也可以与数据进行比较，但不应期望它们和后验预测检查相同的方式进行校准。这需要猜测后验并将其编程到先验中。我们的目标是确保先验值不会太宽泛，从而使概率质量偏离可行值。

## Mixed predictive replication for hierarchical models {#mixed-replication}
分层模型的混合预测重复

@GelmanEtAl:1996 discuss the case of mixed replication for
hierarchical models in which the hyperparameters remain fixed, but
varying effects are replicated.  This is neither a purely prior nor
purely posterior predictive check, but falls somewhere in between.

@GelmanEtAl:1996
讨论了分层模型的混合复制情况，其中超参数保持固定，但复制了不同的效果。这既不是纯粹的先验也不是纯粹的后验预测检查，而是介于两者之间。

For example, consider a simple varying intercept logistic regression,
with intercepts $\alpha_k$ for $k \in 1:K$.  Each data item
$y_n \in \{ 0, 1 \}$ is assumed to correspond to group $kk_n \in 1:K.$
The sampling distribution is thus
$$
y_n \sim \textrm{bernoulli}(\textrm{logit}^{-1}(\alpha_{kk[n]})).
$$
The varying intercepts have a hierarchical normal prior,
$$
\alpha_k \sim \textrm{normal}(\mu, \sigma).
$$
The hyperparameters are themselves given weakly informative priors,
\begin{eqnarray*}
\mu & \sim & \textrm{normal}(0, 2)
\\[4pt]
\sigma & \sim & \textrm{lognormal}(0, 1).
\end{eqnarray*}

Like in a posterior predictive check, the hyperparameters $\mu$ and
$\sigma$ are drawn from the posterior,
$$
\mu^{(m)}, \sigma^{(m)} \sim p(\mu, \sigma \mid y)
$$
Like in a prior predictive check, replicated values of $\alpha$ are
drawn from the hyperparameters,
$$
\alpha^{\textrm{rep}(m)}_k \sim \textrm{normal}(\alpha_k \mid
\mu^{(m)}, \sigma^{(m)}).
$$
The data items are then each replicated using the replicated intercepts,
$$
y^{\textrm{rep}(m)}_n \sim
\textrm{bernoulli}
  (\textrm{logit}^{-1}(\alpha^{\textrm{rep}(m)}_{kk[n]})).
$$
Thus the $y^{\textrm{rep}(m)}$ can be seen as a kind of posterior
predictive replication of observations from new groups that were not
among the original $K$ groups.

In Stan, mixed predictive replications $y^{\textrm{rep}(m)}$ can be
programmed directly.

```stan
data {
  int<lower=0> K;
  int<lower=0> N;
  array[N] int<lower=1, upper=K> kk;
  array[N] int<lower=0, upper=1> y;
}
parameters {
  real mu;
  real<lower=0> sigma;
  vector<offset=mu, multiplier=sigma>[K] alpha;
}
model {
  mu ~ normal(0, 2);               // hyperprior
  sigma ~ lognormal(0, 1);
  alpha ~ normal(mu, sigma);       // hierarchical prior
  y ~ bernoulli_logit(alpha[kk]);  // sampling distribution
}
generated quantities {
  // alpha replicated;  mu and sigma not replicated
  array[K] real alpha_rep
    = normal_rng(rep_vector(mu, K), sigma);
  array[N] int<lower=0, upper=1> y_rep
    = bernoulli_logit_rng(alpha_rep[kk]);
}
```


## Joint model representation

Following @GelmanEtAl:1996, prior, posterior, and mixed replications
may all be defined as posteriors from joint models over parameters and
observed and replicated data.

### Posterior predictive model

For example, posterior predictive replication may be formulated
using sampling notation as follows.
\begin{eqnarray*}
\theta & \sim & p(\theta)
\\[2pt]
y & \sim & p(y \mid \theta)
\\[2pt]
y^{\textrm{rep}} & \sim & p(y \mid \theta)
\end{eqnarray*}
The heavily overloaded sampling notation is meant to indicate that
both $y$ and $y^{\textrm{rep}}$ are drawn from the same distribution,
or more formally using capital letters to distinguish random
variables, that the conditional densities $p_{Y^{\textrm{rep}} \mid
\Theta}$ and $p_{Y \mid \Theta}$ are the same.

The joint density is
$$
p(\theta, y, y^{\textrm{rep}})
= p(\theta) \cdot p(y \mid \theta) \cdot p(y^{\textrm{rep}} \mid \theta).
$$
This again is assuming that the two distributions for $y$ and
$y^{\textrm{rep}}$ are identical.

The variable $y$ is observed, with the predictive simulation
$y^{\textrm{rep}}$ and parameter vector $\theta$ not observed.  The
posterior is $p(y^{\textrm{rep}}, \theta \mid y)$.  Given draws from
the posterior, the posterior predictive simulations $y^{\textrm{rep}}$
are retained.

### Prior predictive model

The prior predictive model simply drops the data component of the
posterior predictive model.  
\begin{eqnarray*}
\theta & \sim & p(\theta)
\\[2pt]
y^{\textrm{rep}} & \sim & p(y \mid \theta)
\end{eqnarray*}
This corresponds to the joint density
$$
p(\theta, y^{\textrm{rep}}) = p(\theta) \cdot p(y^{\textrm{rep}} \mid
\theta).
$$

It is typically straightforward to draw $\theta$ from the prior and
$y^{\textrm{rep}}$ from the sampling distribution given $\theta$
efficiently.  In cases where it is not, the model may be coded and
executed just as the posterior predictive model, only with no data.

### Mixed replication for hierarchical models

The mixed replication corresponds to the model
\begin{eqnarray*}
\phi & \sim & p(\phi)
\\[2pt]
\alpha & \sim & p(\alpha \mid \phi)
\\[2pt]
y & \sim & p(y \mid \alpha)
\\[2pt]
\alpha^{\textrm{rep}} & \sim & p(\alpha \mid \phi)
\\[2pt]
y^{\textrm{rep}} & \sim & p(y \mid \phi)
\end{eqnarray*}
The notation here is meant to indicate that $\alpha$ and
$\alpha^{\textrm{rep}}$ have identical distributions, as do $y$ and
$y^{\textrm{rep}}$.

This corresponds to a joint model
$$
p(\phi, \alpha, \alpha^{\textrm{rep}}, y, y^{\textrm{rep}})
=
p(\phi)
\cdot p(\alpha \mid \phi)
\cdot p(y \mid \alpha)
\cdot p(\alpha^{\textrm{rep}} \mid \phi)
\cdot p(y^{\textrm{rep}} \mid \alpha^{\textrm{rep}}),
$$
where $y$ is the only observed variable, $\alpha$ contains the
lower-level parameters and $\phi$ the hyperparameters.  Note that
$\phi$ is not replicated and instead appears in the distribution for
both $\alpha$ and $\alpha^{\textrm{rep}}$.

The posterior is $p(\phi, \alpha, \alpha^{\textrm{rep}},
y^{\textrm{rep}} \mid y)$.  From posterior draws, the posterior
predictive simulations $y^{\textrm{rep}}$ are kept.
